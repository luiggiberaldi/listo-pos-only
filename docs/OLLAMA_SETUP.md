# Ollama Setup Guide - Listo Ghost Edge Consciousness

## ¬øQu√© es Ollama?

Ollama es un runtime local para modelos de lenguaje (LLMs) que te permite ejecutar inteligencia artificial directamente en tu computadora, sin depender de servicios cloud externos.

**Beneficios para Listo Ghost:**
- ‚úÖ **Costo $0** - Sin l√≠mites de API ni cuotas mensuales
- ‚úÖ **Latencia Ultra-Baja** - Respuestas en 50-200ms vs 500-2000ms cloud
- ‚úÖ **Privacidad Total** - Tus consultas nunca salen de tu m√°quina
- ‚úÖ **Disponibilidad Offline** - Funciona sin conexi√≥n a internet

---

## Instalaci√≥n (Windows)

### 1. Descargar Ollama

Visita: **https://ollama.ai/download**

Descarga el instalador para Windows y ejec√∫talo. El proceso es autom√°tico.

### 2. Verificar Instalaci√≥n

Abre **PowerShell** o **CMD** y ejecuta:

```bash
ollama --version
```

Deber√≠as ver algo como: `ollama version 0.1.x`

### 3. Descargar el Modelo Recomendado

Ejecuta el siguiente comando para descargar **Llama 3.1 8B** (4.7GB):

```bash
ollama pull llama3.1:8b
```

**Tiempo estimado:** 5-15 minutos dependiendo de tu conexi√≥n.

**Alternativa ligera** (si tienes hardware limitado):
```bash
ollama pull phi3:mini
```
(Solo 2GB, pero menos preciso)

### 4. Verificar que el Modelo Est√° Listo

```bash
ollama list
```

Deber√≠as ver `llama3.1:8b` en la lista.

### 5. Probar el Modelo (Opcional)

```bash
ollama run llama3.1:8b
```

Escribe cualquier pregunta y presiona Enter. Para salir, escribe `/bye`.

---

## Configuraci√≥n en Listo POS

### Variables de Entorno

Abre tu archivo `.env` y agrega (o verifica que existan):

```env
# Ollama Local LLM (Opcional - Prioridad M√°xima)
VITE_OLLAMA_ENDPOINT=http://localhost:11434
VITE_OLLAMA_MODEL=llama3.1:8b
```

### Reiniciar la Aplicaci√≥n

1. Det√©n el servidor de desarrollo (`Ctrl+C`)
2. Ejecuta `npm run dev` nuevamente
3. Abre la aplicaci√≥n en el navegador

### Verificar que Ollama Est√° Activo

En el chat de **Listo Ghost**, observa el LED de estado:

- üü° **Dorado/Amarillo**: Ollama Local activo (Poder Ilimitado)
- üîµ **Cyan**: Local Reasoner (Determin√≠stico)
- üü£ **Violeta**: Gemini Cloud (Fallback)

---

## Troubleshooting

### ‚ùå "Ollama no se detecta"

**Causa:** El servicio de Ollama no est√° corriendo.

**Soluci√≥n:**
1. Abre **Servicios de Windows** (`services.msc`)
2. Busca "Ollama Service"
3. Aseg√∫rate de que est√© en estado "Running"
4. Si no existe, reinstala Ollama

### ‚ùå "Puerto 11434 ocupado"

**Causa:** Otro proceso est√° usando el puerto de Ollama.

**Soluci√≥n:**
```bash
netstat -ano | findstr :11434
```
Identifica el PID y termina el proceso en el Administrador de Tareas.

### ‚ùå "Modelo no encontrado"

**Causa:** No descargaste el modelo o usaste un nombre incorrecto.

**Soluci√≥n:**
```bash
ollama pull llama3.1:8b
```

### ‚ùå "Respuestas muy lentas"

**Causa:** Hardware insuficiente (RAM < 8GB).

**Soluci√≥n:**
- Usa el modelo ligero: `ollama pull phi3:mini`
- Actualiza `.env` con `VITE_OLLAMA_MODEL=phi3:mini`

---

## Especificaciones T√©cnicas

### Modelo Recomendado: Llama 3.1 8B

- **Tama√±o:** 4.7GB
- **RAM Requerida:** 8GB m√≠nimo (16GB recomendado)
- **Contexto:** 128K tokens
- **Velocidad:** ~50-200ms por respuesta
- **Calidad:** Comparable a GPT-3.5

### API Endpoint

Ollama expone una API REST en `http://localhost:11434`:

**Verificar disponibilidad:**
```bash
curl http://localhost:11434/api/tags
```

**Generar respuesta:**
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "¬øC√≥mo funciona el login en Listo POS?",
  "stream": false
}'
```

---

## Notas Importantes

> [!WARNING]
> **Privacidad de Datos**
> Aunque Ollama es local, aseg√∫rate de no compartir informaci√≥n sensible en tus prompts si planeas usar modelos cloud como fallback.

> [!TIP]
> **Optimizaci√≥n de Rendimiento**
> - Cierra aplicaciones pesadas mientras usas Ollama
> - Considera usar un SSD para mejorar la velocidad de carga del modelo
- Si tienes GPU NVIDIA, Ollama la usar√° autom√°ticamente para acelerar las respuestas

---

## Soporte

Si tienes problemas con Ollama, visita:
- **Documentaci√≥n oficial:** https://ollama.ai/docs
- **GitHub:** https://github.com/ollama/ollama
- **Discord:** https://discord.gg/ollama
